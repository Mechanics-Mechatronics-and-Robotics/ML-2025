{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avkornaev/ICML-2025/blob/main/UQ_CIFAR-10N_ProCoSphere.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dviRg5xn5mza"
      },
      "source": [
        "#Uncertainty Quantification with CIFAR-10N and Ensembling\n",
        "\n",
        "By *First name* *Second name*.\n",
        "\n",
        "*Month, Day, 2025.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS8QkXU47uNl"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvP6IWSB5s_c"
      },
      "source": [
        "Re-annotated versions of the CIFAR-10 and CIFAR-100 data which contains real-world human annotation errors. The noise patterns deviate from the classically assumed ones and what the new challenges are. The website of CIFAR-N is available at [cifar-10-100n\n",
        "](https://github.com/UCSC-REAL/cifar-10-100n/tree/main) project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN9yU_U6766c"
      },
      "source": [
        "# Preparation of simulation models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mpwlpoX5TI3"
      },
      "source": [
        "## Import and Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install --upgrade torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "xMPnV0nV4_-I"
      },
      "outputs": [],
      "source": [
        "# !pip install pytorch-lightning clearml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fDHrafErmo43"
      },
      "outputs": [],
      "source": [
        "#Pytorch modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.transforms import RandAugment\n",
        "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
        "from torch.utils.data import Subset  # <-- Fix missing import\n",
        "#scipy\n",
        "from scipy.stats import mode\n",
        "#sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torchmetrics import CalibrationError\n",
        "#Numpy\n",
        "import numpy as np\n",
        "from numpy.core.multiarray import _reconstruct\n",
        "#Pandas\n",
        "import pandas as pd\n",
        "#Lightning & logging\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "#Data observation\n",
        "from PIL import Image\n",
        "import random\n",
        "import os\n",
        "# import sys\n",
        "# import pickle\n",
        "import requests\n",
        "from pathlib import Path\n",
        "#Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "#Logging\n",
        "from clearml import Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential safety settings\n",
        "torch.serialization.add_safe_globals([_reconstruct, np.ndarray])\n",
        "\n",
        "# Disable ClearML's torch.load patching temporarily\n",
        "from clearml.binding.frameworks import _patched_call\n",
        "original_torch_load = torch.load\n",
        "_patched_call._original_torch_load = original_torch_load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ealb85K93wDT"
      },
      "source": [
        "## Set the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHIKBWI93-zD"
      },
      "source": [
        "### Simulation Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiYPAzh54gjM"
      },
      "source": [
        "Check the current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "phE7U1vu31BR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/project/ICML-2025/ICML-2025'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd() #returns the current working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7EVwTLDiNYyc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CHECKPOINT_PATH: /project/ICML-2025/ICML-2025/../saved_models/\n"
          ]
        }
      ],
      "source": [
        "# Go one level up (outside the current directory)\n",
        "parent_dir = os.path.join(os.getcwd(), os.pardir)\n",
        "\n",
        "# Set the checkpoint path to a folder in the parent directory\n",
        "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", os.path.join(parent_dir, \"saved_models/\"))\n",
        "print(f'CHECKPOINT_PATH: {CHECKPOINT_PATH}')\n",
        "\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WK77wcO6sfb"
      },
      "source": [
        "Set the reproducibility options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2YX7JeP93-TZ"
      },
      "outputs": [],
      "source": [
        "# Function for setting the seed to implement parallel tests\n",
        "SEEDS =  [42]#[42, 0, 17, 9, 3, 16, 2]\n",
        "SEED = 42 # random seed by default\n",
        "# pl.seed_everything(SEED)\n",
        "\n",
        "# Determine the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    pl.seed_everything(seed)\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(seed)\n",
        "    return g\n",
        "\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7ULmzow4jSg"
      },
      "source": [
        "### Logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C97DLT0gK37A"
      },
      "source": [
        "To configure ClearML in your Colab environment, follow these steps:\n",
        "\n",
        "---\n",
        "\n",
        "*Step 1: Create a ClearML Account*\n",
        "1. Go to the [ClearML website](https://clear.ml/).\n",
        "2. Sign up for a free account if you donâ€™t already have one.\n",
        "3. Once registered, log in to your ClearML account.\n",
        "\n",
        "---\n",
        "\n",
        "*Step 2: Get Your ClearML Credentials*\n",
        "1. After logging in, navigate to the **Settings** page (click on your profile icon in the top-right corner and select **Settings**).\n",
        "2. Under the **Workspace** section, find your **+ Create new credentials**.\n",
        "3. Copy these credentials for a Jupiter notebook into the code cell below.\n",
        "\n",
        "---\n",
        "\n",
        "*Step 3: Accessing the ClearML Dashboard*\n",
        "1. Go to your ClearML dashboard (https://app.clear.ml).\n",
        "2. Navigate to the **Projects** section to see your experiments.\n",
        "3. Click on the experiment (e.g., `Lab_1`) to view detailed metrics, logs, and artifacts.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lTXMGNya32_3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=ZP02U03C6V5ER4K9VWRNZT7EWA5ZTV\n",
            "env: CLEARML_API_SECRET_KEY=BtA5GXZufr6QGpaqhX1GSKPTvaCt56OLqaNqUGLNoxx2Ye8Ctwbui0Ln5OXVnzUgH4I\n"
          ]
        }
      ],
      "source": [
        "#Enter your code here to implement Step 2 of the logging instruction as it is shown below\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=ZP02U03C6V5ER4K9VWRNZT7EWA5ZTV\n",
        "%env CLEARML_API_SECRET_KEY=BtA5GXZufr6QGpaqhX1GSKPTvaCt56OLqaNqUGLNoxx2Ye8Ctwbui0Ln5OXVnzUgH4I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BujHK4sw7cA7"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb0uJtxz-E--"
      },
      "source": [
        "Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hWRDBJbO7k_u"
      },
      "outputs": [],
      "source": [
        "DATASET = 'CIFAR10N' # dataset with the real-world noise\n",
        "# Can be 'clean_label', 'worse_label', 'aggre_label', 'random_label1', 'random_label2', 'random_label3'\n",
        "NOISE_TYPE = 'worse_label'\n",
        "LS = 0.0\n",
        "\n",
        "SIZE = 32 #image size 32 is original size, but a ViT needs 224\n",
        "NUM_CLASSES = 10\n",
        "CLASS_NAMES = ['plane', 'car', 'bird', 'cat',\n",
        "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwaBDKEvD5ya"
      },
      "source": [
        "### Collect parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NSyUzYCSD0v3"
      },
      "outputs": [],
      "source": [
        "#Model parameters\n",
        "LOSS_FUN = 'ProCoSphere' # 'CE','CELoss'(custom), 'N', 'B', etc.\n",
        "ARCHITECTURE = 'ProCoSphereEncoder' # 'CNN, 'ResNet50', 'ViT', etc.\n",
        "LAMBDA = 0.5\n",
        "#Collect the parameters (hyperparams and others)\n",
        "# im_size = SIZE if ARCHITECTURE == 'CNN' else 224\n",
        "hparams = {\n",
        "    \"seed\": SEED,\n",
        "    \"lr\": 1E-3, #0.001,\n",
        "    'weight_decay': 5.0E-4,\n",
        "    \"dropout\": {'dropout_rate': 0.1, 'mc_samples': 10},\n",
        "    \"embed_dim\": 10, #NUM_CLASSES, #128,   #Embedding size D\n",
        "    \"lambda_con\": LAMBDA,\n",
        "    \"temperature\": 0.1,  # For contrastive loss if needed\n",
        "    \"bs\": 500, #256,#32,\n",
        "    \"num_workers\": 10,\n",
        "    \"num_epochs\": 10,\n",
        "    \"warmup_epochs\": 0,\n",
        "    \"selective_sampling\": True,\n",
        "    \"selection_threshold\": 0.1,\n",
        "    \"criterion\": LOSS_FUN,\n",
        "    \"architecture\": ARCHITECTURE,\n",
        "    \"freeze\": False,\n",
        "    \"train_ratio\": 0.9,\n",
        "    \"im_size\": SIZE,\n",
        "    \"mean\": [0.4914, 0.4822, 0.4465],\n",
        "    \"std\": [0.2470, 0.2435, 0.2616],\n",
        "    'randResCrop': {'size': (SIZE, SIZE), 'scale': (0.8, 1.0), 'ratio': (0.9, 1.1)},\n",
        "    'label_smoothing': LS,\n",
        "    \"n_classes\": NUM_CLASSES,\n",
        "    \"noise_path\": './data/CIFAR-10_human.pt',\n",
        "    \"noise_type\": NOISE_TYPE,  # Can be 'clean_label', 'worse_label', 'aggre_label', etc.\n",
        "    \"resume_checkpoint\": None,#'/project/ICML-2025/saved_models/arch_ProCoSphereEncoder_loss_ProCoSphere_lambda_0.5_seed_9_noise_clean_label-v1.ckpt',\n",
        "}\n",
        "\n",
        "#Visualization\n",
        "vis_params = {\n",
        "    'fig_size': 5,\n",
        "    'num_samples': 5,\n",
        "    'num_bins': 50,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQsu5FcbFfwx"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAOMnXlqFofC"
      },
      "source": [
        "### Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N2vslXcUvGT"
      },
      "source": [
        "Data module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l4LT-NyRgAWZ"
      },
      "outputs": [],
      "source": [
        "def download_file(url, save_path):\n",
        "    \"\"\"Download a file from a URL and save it to the specified path.\"\"\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure directory exists\n",
        "        with open(save_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"File downloaded and saved to {save_path}\")\n",
        "    else:\n",
        "        raise Exception(f\"Failed to download file from {url}. Status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_3lxFLivgJIn"
      },
      "outputs": [],
      "source": [
        "class CIFAR10(datasets.CIFAR10):\n",
        "    \"\"\"CIFAR10 dataset with noisy labels and dual views\"\"\"\n",
        "    def __init__(self, root, train=True, transform_clf=None, transform_agn=None,\n",
        "                 target_transform=None, download=False, noise_type=None,\n",
        "                 noise_path=None, is_human=True):\n",
        "        super().__init__(root, train=train, transform=None,  # Disable default transform\n",
        "                         target_transform=target_transform, download=download)\n",
        "        self.noise_type = noise_type\n",
        "        self.noise_path = noise_path\n",
        "        self.is_human = is_human\n",
        "        self.transform_clf = transform_clf\n",
        "        self.transform_agn = transform_agn\n",
        "\n",
        "        if self.train and self.noise_type is not None:\n",
        "            self.load_noisy_labels()\n",
        "\n",
        "    def load_noisy_labels(self):\n",
        "        from numpy.core.multiarray import _reconstruct\n",
        "        import torch.serialization\n",
        "        torch.serialization.add_safe_globals([_reconstruct])\n",
        "\n",
        "        noise_file = torch.load(self.noise_path, map_location='cpu', weights_only=False)\n",
        "        if isinstance(noise_file, dict):\n",
        "            if \"clean_label\" in noise_file.keys():\n",
        "                clean_label = torch.tensor(noise_file['clean_label'])\n",
        "                assert torch.sum(torch.tensor(self.targets) - clean_label) == 0\n",
        "                print(f'Loaded {self.noise_type} from {self.noise_path}.')\n",
        "                print(f'Noise rate: {1 - np.mean(clean_label.numpy() == noise_file[self.noise_type])}')\n",
        "            self.noisy_labels = noise_file[self.noise_type].reshape(-1)\n",
        "        else:\n",
        "            raise Exception('Invalid noise file format')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        # Apply dual transforms\n",
        "        if self.transform_clf:\n",
        "            clf_view = self.transform_clf(img)\n",
        "        if self.transform_agn:\n",
        "            agn_view = self.transform_agn(img)\n",
        "\n",
        "        # Apply noisy labels if training\n",
        "        if self.train and self.noise_type is not None:\n",
        "            target = self.noisy_labels[index]\n",
        "\n",
        "        return {\n",
        "            'clf_view': clf_view,\n",
        "            'agn_view': agn_view,\n",
        "            'target': target,\n",
        "            'index': index\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(params)\n",
        "        self.seed = params['seed']\n",
        "        self.batch_size = params['bs']\n",
        "        self.num_workers = min(params['num_workers'], 4)\n",
        "        self.mean = params['mean']\n",
        "        self.std = params['std']\n",
        "        self.train_ratio = params['train_ratio']\n",
        "        self.rand_res_crop = params['randResCrop']\n",
        "        self.noise_path = params.get('noise_path', './data/CIFAR-10_human.pt')\n",
        "        self.noise_type = params.get('noise_type', 'worse_label')\n",
        "        self.im_size = params.get('im_size', 32)\n",
        "        self.embed_dim = params.get('embed_dim', 128)\n",
        "        self.full_train = None\n",
        "        self.original_train_indices = None\n",
        "        self.original_val_indices = None\n",
        "        self.train_mask = None\n",
        "        self.val_mask = None\n",
        "        self.clean_labels = None\n",
        "        self.noisy_labels = None\n",
        "\n",
        "        # Transforms for classifier (train/val)\n",
        "        self.clf_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(\n",
        "                size=self.rand_res_crop['size'],\n",
        "                scale=self.rand_res_crop['scale'],\n",
        "                ratio=self.rand_res_crop['ratio']\n",
        "            ),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            RandAugment(num_ops=2, magnitude=9),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std)\n",
        "        ])\n",
        "\n",
        "        # Transforms for agnostic views (contrastive learning)\n",
        "        self.agn_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(\n",
        "                size=self.rand_res_crop['size'],\n",
        "                scale=(0.3, 1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "            transforms.GaussianBlur(3),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std)\n",
        "        ])\n",
        "\n",
        "        # Simplified test transform: Only RandomResizedCrop + ToTensor\n",
        "        self.test_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(\n",
        "                size=self.rand_res_crop['size'],\n",
        "                scale=(0.9, 1.0),\n",
        "                ratio=self.rand_res_crop['ratio']\n",
        "            ),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std)\n",
        "        ])\n",
        "\n",
        "        os.makedirs(os.path.dirname(self.noise_path), exist_ok=True)\n",
        "        self._download_data()\n",
        "\n",
        "    def _download_data(self):\n",
        "        if not os.path.exists(self.noise_path):\n",
        "            download_file(\n",
        "                \"https://github.com/UCSC-REAL/cifar-10-100n/raw/main/data/CIFAR-10_human.pt\",\n",
        "                self.noise_path\n",
        "            )\n",
        "\n",
        "    def prepare_data(self):\n",
        "        datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "        datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Train + val\n",
        "        full_train = CIFAR10(\n",
        "            root='./data',\n",
        "            train=True,\n",
        "            transform_clf=self.clf_transform,\n",
        "            transform_agn=self.agn_transform,\n",
        "            noise_type=self.noise_type,\n",
        "            noise_path=self.noise_path\n",
        "        )\n",
        "        self.full_train = full_train\n",
        "        self.clean_labels = torch.tensor(full_train.targets)\n",
        "        self.noisy_labels = full_train.noisy_labels\n",
        "\n",
        "        # Create original splits\n",
        "        full_size = len(self.full_train)\n",
        "        g = seed_everything(self.seed)\n",
        "        indices = torch.randperm(full_size, generator=g)\n",
        "        train_size = int(full_size * self.train_ratio)\n",
        "        self.original_train_indices = indices[:train_size]\n",
        "        self.original_val_indices = indices[train_size:]\n",
        "    \n",
        "        # Initialize masks (all True initially)\n",
        "        self.train_mask = torch.ones_like(self.original_train_indices, dtype=torch.bool)\n",
        "        self.val_mask = torch.ones_like(self.original_val_indices, dtype=torch.bool)\n",
        "        self._create_splits()\n",
        "\n",
        "        # Test uses simplified transform (no noise or normalization)\n",
        "        self.cifar10_test = CIFAR10(\n",
        "            root='./data',\n",
        "            train=False,\n",
        "            transform_clf=self.test_transform,  # Use simplified transform\n",
        "            transform_agn=self.test_transform,  # Optional: Can be different\n",
        "            noise_type=None,\n",
        "            noise_path=self.noise_path\n",
        "        )\n",
        "\n",
        "    def _create_splits(self):\n",
        "        \"\"\"Create subsets using current masks\"\"\"\n",
        "        active_train = Subset(self.full_train, \n",
        "                            self.original_train_indices[self.train_mask])\n",
        "        active_val = Subset(self.full_train, \n",
        "                          self.original_val_indices[self.val_mask])\n",
        "        \n",
        "        self.cifar10_train = active_train\n",
        "        self.cifar10_val = active_val\n",
        "        print(f\"\\nActive training samples: {len(self.cifar10_train)}/{len(self.original_train_indices)}\")\n",
        "        print(f\"Active validation samples: {len(self.cifar10_val)}/{len(self.original_val_indices)}\")\n",
        "\n",
        "    def update_masks(self, new_train_mask, new_val_mask):\n",
        "        self.train_mask = new_train_mask.clone()\n",
        "        self.val_mask = new_val_mask.clone()\n",
        "        self._create_splits()\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self._create_dataloader(self.cifar10_train, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self._create_dataloader(self.cifar10_val)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return self._create_dataloader(self.cifar10_test)\n",
        "\n",
        "    def _create_dataloader(self, dataset, shuffle=False):\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            worker_init_fn=seed_worker,\n",
        "            shuffle=shuffle,\n",
        "            persistent_workers=self.num_workers > 0,\n",
        "            pin_memory=True,\n",
        "            drop_last=shuffle\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def robust_load_pl_model(checkpoint_path, model_class, device, **init_kwargs):\n",
        "    \"\"\"Universal PyTorch Lightning model loader that handles:\n",
        "    - PyTorch 2.6+ security restrictions\n",
        "    - ClearML interference\n",
        "    - Numpy array compatibility\n",
        "    \"\"\"\n",
        "    # Bypass all patching and load raw checkpoint\n",
        "    checkpoint = original_torch_load(\n",
        "        checkpoint_path,\n",
        "        map_location=device,\n",
        "        weights_only=False\n",
        "    )\n",
        "    \n",
        "    # Reconstruct model manually\n",
        "    model = model_class(**init_kwargs)\n",
        "    \n",
        "    # Handle both regular and PyTorch Lightning checkpoint formats\n",
        "    if 'state_dict' in checkpoint:\n",
        "        state_dict = checkpoint['state_dict']\n",
        "    elif 'model' in checkpoint:\n",
        "        state_dict = checkpoint['model']\n",
        "    else:\n",
        "        state_dict = checkpoint\n",
        "    \n",
        "    # Load state dict with strict=False for compatibility\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    return model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBWSuYApFu7X"
      },
      "source": [
        "Training module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_wXhmaCP6pvJ"
      },
      "outputs": [],
      "source": [
        "class train_model(pl.LightningModule):\n",
        "    def __init__(self, model=None, loss=None, hparams=hparams):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "        self.model = model\n",
        "        self.loss_fn = loss\n",
        "        self.automatic_optimization = False\n",
        "        self.training_losses = []\n",
        "        self.training_indices = []\n",
        "        self.validation_losses = []\n",
        "        self.validation_indices = []\n",
        "        self.current_phase = 'warmup'\n",
        "\n",
        "     # Add this critical method PROPERLY INDENTED under the class\n",
        "    def configure_optimizers(self):\n",
        "        base_lr = self.hparams['lr']\n",
        "        warmup_lr = self.hparams['lr']\n",
        "\n",
        "        # 3 parameter groups\n",
        "        params = [\n",
        "            {'params': self.model.agnostic.parameters(), 'lr': warmup_lr},\n",
        "            {'params': self.model.classifier.parameters(), \n",
        "            'lr': base_lr,\n",
        "            'weight_decay': self.hparams['weight_decay']},\n",
        "            {'params': self.loss_fn.parameters(), 'lr': base_lr}\n",
        "        ]\n",
        "\n",
        "        optimizer = torch.optim.AdamW(params)\n",
        "\n",
        "        # # Define separate lambdas for each parameter group\n",
        "        # def agnostic_lambda(epoch):\n",
        "        #     return min(1.0, 0.01 + (0.99 * epoch / (self.hparams['warmup_epochs'] + 1.0e-6)))\n",
        "    \n",
        "        # classifier_lambda = lambda _: 1.0  # No warmup\n",
        "        # loss_lambda = lambda _: 1.0  # No warmup\n",
        "\n",
        "        # warmup = torch.optim.lr_scheduler.LambdaLR(\n",
        "        #     optimizer,\n",
        "        #     lr_lambda=[agnostic_lambda, classifier_lambda, loss_lambda]\n",
        "        # )\n",
        "\n",
        "        # cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        #     optimizer,\n",
        "        #     T_max=self.hparams['num_epochs'] - self.hparams['warmup_epochs'],\n",
        "        #     eta_min=self.hparams['lr']/100\n",
        "        # )\n",
        "\n",
        "        # scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
        "        #     optimizer,\n",
        "        #     schedulers=[warmup, cosine],\n",
        "        #     milestones=[self.hparams['warmup_epochs']]\n",
        "        # )\n",
        "\n",
        "        return [optimizer] #, [scheduler]\n",
        "    \n",
        "    def forward(self, x_clf, x_agn):\n",
        "        return self.model.classifier(x_clf), self.model.agnostic(x_agn)\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        \"\"\"Handle phase transitions and parameter freezing\"\"\"\n",
        "        # Reset mask at beginning of training\n",
        "        if self.current_epoch == 0:\n",
        "            self.trainer.datamodule.train_mask = torch.ones_like(\n",
        "                self.trainer.datamodule.train_mask, dtype=torch.bool\n",
        "            )\n",
        "            self.trainer.datamodule.val_mask = torch.ones_like(\n",
        "                self.trainer.datamodule.val_mask, dtype=torch.bool\n",
        "            )\n",
        "            self.trainer.datamodule._create_splits()\n",
        "        \n",
        "        self.training_losses.clear()\n",
        "        self.training_indices.clear()\n",
        "        self.validation_losses.clear()\n",
        "        self.validation_indices.clear()\n",
        "        # Add initialization of sample indices list\n",
        "        self.sample_indices = []  # <-- FIX HERE\n",
        "        if self.current_epoch < self.hparams['warmup_epochs']:\n",
        "            self.current_phase = 'warmup'\n",
        "            # Freeze classifier, train agnostic encoder\n",
        "            for param in self.model.classifier.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.model.agnostic.parameters():\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            self.current_phase = 'joint'\n",
        "            # Unfreeze classifier for joint training\n",
        "            for param in self.model.classifier.parameters():\n",
        "                param.requires_grad = True\n",
        "            for param in self.model.agnostic.parameters():\n",
        "                param.requires_grad = True\n",
        "        self.loss_fn.set_phase(self.current_phase)\n",
        "\n",
        "    def _shared_step(self, batch, stage):\n",
        "        \"\"\"Unified processing for validation/test stages\"\"\"\n",
        "        x_clf = batch['clf_view']\n",
        "        x_agn = batch['agn_view']\n",
        "        y = batch['target']\n",
        "\n",
        "        # Always use full model for validation/test\n",
        "        clf_out, agn_out = self(x_clf, x_agn)\n",
        "        loss = self.loss_fn(clf_out, agn_out, y)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Metrics calculation\n",
        "            var_clf = clf_out['log_var'].exp().mean()\n",
        "            var_agn = agn_out['log_var'].exp().mean()\n",
        "            similarity = F.cosine_similarity(clf_out['embed'], agn_out['embed']).mean()\n",
        "\n",
        "            logs = {\n",
        "                f'{stage}_loss': loss,\n",
        "                f'{stage}_var_clf': var_clf,\n",
        "                f'{stage}_var_agn': var_agn,\n",
        "                f'{stage}_sim': similarity\n",
        "            }\n",
        "\n",
        "            if stage == 'test':\n",
        "                preds = clf_out['logits'].argmax(1)\n",
        "                logs[f'{stage}_acc'] = (preds == y).float().mean()\n",
        "\n",
        "        self.log_dict(logs, prog_bar=(stage == 'train'), on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        opt = self.optimizers()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if self.current_phase == 'warmup':\n",
        "            # Warmup phase: contrastive learning between dual views\n",
        "            view1 = batch['agn_view']\n",
        "            view2 = batch['clf_view']\n",
        "            \n",
        "            # Both views through agnostic encoder\n",
        "            agn_out1 = self.model.agnostic(view1)\n",
        "            agn_out2 = self.model.agnostic(view2)\n",
        "            \n",
        "            # Compute contrastive loss\n",
        "            # total_loss = self.loss_fn(agn_out1, agn_out2, batch['target'])\n",
        "            loss_con = self.loss_fn._contrastive_loss(agn_out1['embed'], \n",
        "                                        agn_out2['embed'],\n",
        "                                        agn_out1['log_var'].exp())\n",
        "            total_loss = loss_con\n",
        "        else:\n",
        "            # Joint training phase\n",
        "            clf_out, agn_out = self(batch['clf_view'], batch['agn_view'])\n",
        "            total_loss = self.loss_fn(clf_out, agn_out, batch['target'])\n",
        "            \n",
        "            # Compute per-sample classification loss for selective sampling\n",
        "            with torch.no_grad():\n",
        "                per_sample_cls_loss = self.loss_fn.classification_loss_per_sample(\n",
        "                    clf_out['logits'], batch['target']\n",
        "                )\n",
        "            \n",
        "            self.training_losses.append(per_sample_cls_loss.detach().cpu())\n",
        "            self.sample_indices.append(batch['index'].cpu())\n",
        "\n",
        "        self.manual_backward(total_loss.mean())\n",
        "        opt.step()      \n",
        "        return total_loss.mean()\n",
        "    \n",
        "    def on_train_epoch_end(self):\n",
        "        if self.current_phase == 'joint' and self.hparams['selective_sampling']:\n",
        "            if not self.training_losses:\n",
        "                return\n",
        "        \n",
        "            # Process TRAIN mask first\n",
        "            losses = torch.cat(self.training_losses)\n",
        "            all_indices = torch.cat(self.sample_indices)\n",
        "            gathered_losses = self.all_gather(losses).flatten()\n",
        "            gathered_indices = self.all_gather(all_indices).flatten()\n",
        "\n",
        "            new_mask = None\n",
        "            new_val_mask = self.trainer.datamodule.val_mask.clone()\n",
        "\n",
        "            if self.trainer.is_global_zero:\n",
        "                dm = self.trainer.datamodule\n",
        "                current_train_mask = dm.train_mask\n",
        "                original_train_indices = dm.original_train_indices\n",
        "\n",
        "                # Create mapping from original index to its position in original_train_indices\n",
        "                index_to_pos = {idx.item(): pos for pos, idx in enumerate(original_train_indices)}\n",
        "\n",
        "                # Convert subset indices (original indices) to positions in original_train_indices\n",
        "                subset_indices_np = gathered_indices.cpu().numpy()\n",
        "                subset_positions = [index_to_pos.get(idx, -1) for idx in subset_indices_np]\n",
        "\n",
        "                # Filter valid positions (indices present in original_train_indices)\n",
        "                valid_mask = np.array([pos != -1 for pos in subset_positions])\n",
        "                valid_positions = np.array(subset_positions)[valid_mask]\n",
        "                valid_losses = gathered_losses[valid_mask]\n",
        "\n",
        "                # Initialize loss tensor with infinity (inactive samples have high loss)\n",
        "                per_sample_loss = torch.full(\n",
        "                    (len(original_train_indices),),  # Correct tuple format\n",
        "                    float('inf'), \n",
        "                    device=gathered_losses.device\n",
        "                )\n",
        "                per_sample_loss[valid_positions] = valid_losses\n",
        "\n",
        "                # Compute threshold based on active samples' losses\n",
        "                threshold = torch.quantile(valid_losses, self.hparams['selection_threshold'])\n",
        "            \n",
        "                # Update train mask: keep samples where loss < threshold\n",
        "                new_mask = (per_sample_loss < threshold).cpu()\n",
        "\n",
        "                # Process VALIDATION mask\n",
        "                if self.validation_losses:\n",
        "                    val_losses = torch.cat(self.validation_losses)\n",
        "                    val_indices = torch.cat(self.validation_indices)\n",
        "                    gathered_val_losses = self.all_gather(val_losses).flatten()\n",
        "                    gathered_val_indices = self.all_gather(val_indices).flatten()\n",
        "\n",
        "                    val_subset_indices_np = gathered_val_indices.cpu().numpy()\n",
        "                    val_subset_positions = [index_to_pos.get(idx, -1) for idx in val_subset_indices_np]\n",
        "            \n",
        "                    val_valid_mask = np.array([pos != -1 for pos in val_subset_positions])\n",
        "                    val_valid_positions = np.array(val_subset_positions)[val_valid_mask]\n",
        "                    val_valid_losses = gathered_val_losses[val_valid_mask]\n",
        "\n",
        "                    # Key fix: Add empty tensor check for validation losses\n",
        "                    if len(val_valid_losses) > 0:\n",
        "                        per_val_loss = torch.full(\n",
        "                            (len(dm.original_train_indices),),\n",
        "                            float('inf'),\n",
        "                            device=gathered_val_losses.device\n",
        "                        )\n",
        "                        per_val_loss[val_valid_positions] = val_valid_losses\n",
        "                        val_thresh = torch.quantile(val_valid_losses, self.hparams['selection_threshold'])\n",
        "                        new_val_mask = (per_val_loss < val_thresh).cpu()\n",
        "                    else:\n",
        "                        # Handle empty validation losses case\n",
        "                        new_val_mask = self.val_mask.clone()\n",
        "                        val_thresh = torch.tensor(float('inf'))\n",
        "                else:\n",
        "                    # No validation losses collected\n",
        "                    new_val_mask = self.val_mask.clone()\n",
        "                    val_thresh = torch.tensor(float('inf'))\n",
        "\n",
        "                # Apply new masks\n",
        "                dm.update_masks(new_mask, new_val_mask)\n",
        "                self.log('train/selection_ratio', new_mask.float().mean())\n",
        "                self.log('val/selection_ratio', new_val_mask.float().mean())\n",
        "\n",
        "            # Reset collections\n",
        "            self.training_losses.clear()\n",
        "            self.training_indices.clear()\n",
        "            self.validation_losses.clear()\n",
        "            self.validation_indices.clear()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # Perform standard validation step\n",
        "        loss = self._shared_step(batch, 'val')\n",
        "\n",
        "        if self.current_phase == 'joint' and self.hparams['selective_sampling']:\n",
        "            # Compute per-sample losses for validation set selection\n",
        "            with torch.no_grad():\n",
        "                x_clf = batch['clf_view']\n",
        "                x_agn = batch['agn_view']\n",
        "                y = batch['target']\n",
        "        \n",
        "                # Get model outputs\n",
        "                clf_out, agn_out = self(x_clf, x_agn)\n",
        "        \n",
        "                # Calculate per-sample classification loss (same as training)\n",
        "                per_sample_loss = self.loss_fn.classification_loss_per_sample(\n",
        "                    clf_out['logits'], y\n",
        "                )\n",
        "                per_val_loss = torch.full(\n",
        "                    (len(self.trainer.datamodule.original_train_indices),),  # Corrected line\n",
        "                    float('inf'), \n",
        "                    device=per_sample_loss.device\n",
        "                )\n",
        "        \n",
        "                # Store losses and indices for mask update\n",
        "                self.validation_losses.append(per_sample_loss.detach().cpu())\n",
        "                self.validation_indices.append(batch['index'].cpu())\n",
        "    \n",
        "        return loss\n",
        "    # def validation_step(self, batch, batch_idx):\n",
        "    #     return self._shared_step(batch, 'val')\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self._shared_step(batch, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'seed': 42,\n",
              " 'lr': 0.001,\n",
              " 'weight_decay': 0.0005,\n",
              " 'dropout': {'dropout_rate': 0.1, 'mc_samples': 10},\n",
              " 'embed_dim': 10,\n",
              " 'lambda_con': 0.5,\n",
              " 'temperature': 0.1,\n",
              " 'bs': 500,\n",
              " 'num_workers': 10,\n",
              " 'num_epochs': 10,\n",
              " 'warmup_epochs': 0,\n",
              " 'selective_sampling': True,\n",
              " 'selection_threshold': 0.1,\n",
              " 'criterion': 'ProCoSphere',\n",
              " 'architecture': 'ProCoSphereEncoder',\n",
              " 'freeze': False,\n",
              " 'train_ratio': 0.9,\n",
              " 'im_size': 32,\n",
              " 'mean': [0.4914, 0.4822, 0.4465],\n",
              " 'std': [0.247, 0.2435, 0.2616],\n",
              " 'randResCrop': {'size': (32, 32), 'scale': (0.8, 1.0), 'ratio': (0.9, 1.1)},\n",
              " 'label_smoothing': 0.0,\n",
              " 'n_classes': 10,\n",
              " 'noise_path': './data/CIFAR-10_human.pt',\n",
              " 'noise_type': 'worse_label',\n",
              " 'resume_checkpoint': None}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hparams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpGtoJicGCJC"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "I-yE6d66gWB4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def call_bn(bn, x):\n",
        "    return bn(x)\n",
        "\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature extractor based on your CNN, modified to output embeddings for ProCoSphere.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channel=3, dropout_rate=0.25):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.c1 = nn.Conv2d(input_channel, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.c2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.c3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.c4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.c5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.c6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.c7 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=0)\n",
        "        self.c8 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=0)\n",
        "        self.c9 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=0)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(128)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.bn7 = nn.BatchNorm2d(512)\n",
        "        self.bn8 = nn.BatchNorm2d(256)\n",
        "        self.bn9 = nn.BatchNorm2d(128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.leaky_relu(call_bn(self.bn1, self.c1(x)), negative_slope=0.01)\n",
        "        h = F.leaky_relu(call_bn(self.bn2, self.c2(h)), negative_slope=0.01)\n",
        "        h = F.leaky_relu(call_bn(self.bn3, self.c3(h)), negative_slope=0.01)\n",
        "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
        "        h = F.dropout2d(h, p=self.dropout_rate)\n",
        "\n",
        "        h = F.leaky_relu(call_bn(self.bn4, self.c4(h)), negative_slope=0.01)\n",
        "        h = F.leaky_relu(call_bn(self.bn5, self.c5(h)), negative_slope=0.01)\n",
        "        h = F.leaky_relu(call_bn(self.bn6, self.c6(h)), negative_slope=0.01)\n",
        "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
        "        h = F.dropout2d(h, p=self.dropout_rate)\n",
        "\n",
        "        h = F.leaky_relu(call_bn(self.bn7, self.c7(h)), negative_slope=0.01)\n",
        "        h = F.leaky_relu(call_bn(self.bn8, self.c8(h)), negative_slope=0.01)\n",
        "        h = F.leaky_relu(call_bn(self.bn9, self.c9(h)), negative_slope=0.01)\n",
        "\n",
        "        h = F.avg_pool2d(h, kernel_size=h.data.shape[2])\n",
        "        h = h.view(h.size(0), h.size(1))  # Flatten to (B, C)\n",
        "\n",
        "        return h  # feature dimension is 128\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class ProCoSphereEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Updated encoder using the CNNFeatureExtractor.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channel=3, n_outputs=10, embed_dim=128, dropout_rate=0.25, hparams=None):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams or {}\n",
        "        self.backbone = CNNFeatureExtractor(input_channel=input_channel, dropout_rate=dropout_rate)\n",
        "\n",
        "        self.feat_dim = 128  # after CNN feature extractor\n",
        "\n",
        "        # Heads\n",
        "        self.class_head = nn.Linear(embed_dim, n_outputs) if n_outputs > 0 else None\n",
        "        self.var_head = nn.Linear(self.feat_dim, 1)\n",
        "        self.embed_head = nn.Sequential(\n",
        "            nn.Linear(self.feat_dim, embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        outputs = {\n",
        "            'log_var': self.var_head(features),\n",
        "            'embed': F.normalize(self.embed_head(features), p=2, dim=-1)\n",
        "        }\n",
        "        if self.class_head is not None:\n",
        "            outputs['logits'] = self.class_head(outputs['embed'])\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class ProCoSphereDual(nn.Module):\n",
        "    \"\"\"\n",
        "    Dual-encoder using ProCoSphereEncoder with CNN backbone.\n",
        "    \"\"\"\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        n_classes = hparams.get('n_classes', 10)\n",
        "        embed_dim = hparams.get('embed_dim', 128)\n",
        "        dropout_rate = hparams.get('dropout_rate', 0.25)\n",
        "\n",
        "        self.classifier = ProCoSphereEncoder(\n",
        "            input_channel=hparams.get('input_channel', 3),\n",
        "            n_outputs=n_classes,\n",
        "            embed_dim=embed_dim,\n",
        "            dropout_rate=dropout_rate,\n",
        "            hparams=hparams\n",
        "        )\n",
        "        self.agnostic = ProCoSphereEncoder(\n",
        "            input_channel=hparams.get('input_channel', 3),\n",
        "            n_outputs=0,\n",
        "            embed_dim=embed_dim,\n",
        "            dropout_rate=dropout_rate,\n",
        "            hparams=hparams\n",
        "        )\n",
        "\n",
        "    def forward(self, x_j, x_i):\n",
        "        out_J = self.agnostic(x_j)\n",
        "        out_I = self.classifier(x_i)\n",
        "        return out_J, out_I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHm8wnbnGEnA"
      },
      "source": [
        "### Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProCoSphereLoss(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.num_classes = hparams['n_classes']\n",
        "        self.embed_dim = hparams['embed_dim']\n",
        "        self.lambda_con = hparams['lambda_con']\n",
        "        self.training_phase = 'warmup'  # Initialize with warmup phase\n",
        "        \n",
        "        # Temperature parameters with constrained initialization\n",
        "        self.temp_gain = nn.Parameter(torch.tensor(0.0))  # Start with neutral gain\n",
        "        self.temp_bias = nn.Parameter(torch.tensor(0.1))  # Start with low base temperature\n",
        "        \n",
        "        # Variance constraints\n",
        "        self.log_var_min = -7\n",
        "        self.log_var_max = 4\n",
        "        \n",
        "        # Label smoothing parameters\n",
        "        self.smoothing = hparams.get('label_smoothing', 0.0)\n",
        "        self.contrast_smoothing = 0.01\n",
        "        \n",
        "        # Loss components weights\n",
        "        self.warmup_weight = 1.0\n",
        "        self.cls_weight = 1.0\n",
        "        self.con_weight = hparams['lambda_con']\n",
        "\n",
        "    def set_phase(self, phase):\n",
        "        \"\"\"Dynamically adjust loss components based on training phase\"\"\"\n",
        "        self.training_phase = phase\n",
        "        if phase == 'warmup':\n",
        "            self.warmup_weight = 1.0\n",
        "            self.cls_weight = 0.0\n",
        "            self.con_weight = 1.0\n",
        "        else:\n",
        "            self.warmup_weight = 0.0\n",
        "            self.cls_weight = 1.0\n",
        "            self.con_weight = hparams['lambda_con']\n",
        "\n",
        "    def _contrastive_loss(self, emb1, emb2, variances):\n",
        "        \"\"\"General contrastive loss implementation\"\"\"\n",
        "        # Normalize embeddings\n",
        "        emb1 = F.normalize(emb1, p=2, dim=-1)\n",
        "        emb2 = F.normalize(emb2, p=2, dim=-1)\n",
        "        \n",
        "        # Compute similarity matrix\n",
        "        sim_matrix = emb1 @ emb2.t()\n",
        "        \n",
        "        # Adaptive temperature calculation\n",
        "        temp_per_sample = (self.temp_gain.exp() * variances + self.temp_bias.abs())\n",
        "        temp_per_sample = temp_per_sample.squeeze().clamp(0.01, 100.0)\n",
        "        temp_matrix = torch.sqrt(temp_per_sample.unsqueeze(0) * temp_per_sample.unsqueeze(1))\n",
        "        \n",
        "        # Stabilized similarity\n",
        "        scaled_sim = sim_matrix / temp_matrix.clamp(min=1e-6)\n",
        "        logits_max, _ = torch.max(scaled_sim, dim=1, keepdim=True)\n",
        "        logits_stable = scaled_sim - logits_max.detach()\n",
        "        # logits_stable = sim_matrix\n",
        "        \n",
        "        # Symmetric contrastive loss\n",
        "        targets = torch.arange(emb1.size(0), device=emb1.device)\n",
        "        loss = (F.cross_entropy(logits_stable, targets, label_smoothing=self.contrast_smoothing) +\n",
        "               F.cross_entropy(logits_stable.t(), targets, label_smoothing=self.contrast_smoothing)) / 2\n",
        "               \n",
        "        return loss #torch.clamp(loss, 0.0, 10.0)\n",
        "\n",
        "    def classification_loss_per_sample(self, logits, y):  # Renamed from _classification_loss\n",
        "        \"\"\"Compute per-sample classification loss (without contrastive component).\"\"\"\n",
        "        yoh = torch.zeros_like(logits).scatter(1, y.unsqueeze(1), 1)\n",
        "        yoh = yoh * (1 - self.smoothing) + self.smoothing / self.num_classes\n",
        "        \n",
        "        # Forward CE per sample\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "        ce_forward = -torch.sum(yoh * log_probs, dim=1)  # [batch_size]\n",
        "        \n",
        "        # Reverse CE per sample\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        log_yoh = torch.log(yoh.clamp(min=1e-8))\n",
        "        reverse_ce = -torch.sum(probs * log_yoh, dim=1)  # [batch_size]\n",
        "        \n",
        "        return ce_forward + reverse_ce  # [batch_size]\n",
        "\n",
        "    def forward(self, clf_out, agn_out, y, reduction='mean'):\n",
        "        if self.training_phase == 'warmup':\n",
        "            # Warmup phase: contrastive loss between dual agnostic views\n",
        "            var_agn = torch.exp(torch.clamp(agn_out['log_var'], \n",
        "                                 self.log_var_min, self.log_var_max))\n",
        "            loss_con = self._contrastive_loss(\n",
        "                clf_out['embed'],  # agn_view1 embeddings\n",
        "                agn_out['embed'],   # agn_view2 embeddings\n",
        "                var_agn\n",
        "            )\n",
        "            total_loss = self.warmup_weight * loss_con\n",
        "        else:\n",
        "            # Joint training phase\n",
        "            # Classification loss\n",
        "            loss_cls = self.classification_loss_per_sample(clf_out['logits'], y)\n",
        "\n",
        "            # Contrastive loss\n",
        "            var_agn = torch.exp(torch.clamp(agn_out['log_var'],\n",
        "                               self.log_var_min, self.log_var_max))\n",
        "            loss_con = self._contrastive_loss(\n",
        "                clf_out['embed'],  # classifier embeddings\n",
        "                agn_out['embed'],  # agnostic embeddings\n",
        "                var_agn\n",
        "            )\n",
        "            \n",
        "            total_loss = (self.cls_weight * loss_cls + \n",
        "                         self.con_weight * loss_con)\n",
        "        if reduction == 'none':\n",
        "            return total_loss  # return per-sample losses\n",
        "        else:\n",
        "            return total_loss.mean()  # default behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHSuQd99s_Ou"
      },
      "source": [
        "### Models zoo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0QP_le1tPL1"
      },
      "source": [
        "Architectures and loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WfLgyavidUcW"
      },
      "outputs": [],
      "source": [
        "def get_arch_and_loss(hparams):\n",
        "    if hparams['criterion'] == 'ProCoSphere':\n",
        "        return ProCoSphereDual(hparams), ProCoSphereLoss(hparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWrc-f3sWDDm"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7793mMFKWFYS"
      },
      "outputs": [],
      "source": [
        "def calc_metrics(dataloader, model, hparams):\n",
        "    \"\"\"Compute comprehensive metrics including uncertainty-aware scores\"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Initialize collectors\n",
        "    preds, labels, probs = [], [], []\n",
        "    var_clfs, var_agns, similarities = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x_clf = batch['clf_view'].to(device)\n",
        "            x_agn = batch['agn_view'].to(device)\n",
        "            y = batch['target'].to(device)\n",
        "\n",
        "            # Forward pass through both encoders\n",
        "            clf_out, agn_out = model(x_clf, x_agn)\n",
        "\n",
        "            # Get predictions and probabilities\n",
        "            logits = clf_out['logits']\n",
        "            prob = torch.softmax(logits, dim=1)\n",
        "            pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Collect basic metrics\n",
        "            preds.append(pred.cpu())\n",
        "            labels.append(y.cpu())\n",
        "            probs.append(prob.cpu())\n",
        "\n",
        "            # Collect uncertainty metrics\n",
        "            var_clfs.append(clf_out['log_var'].exp().cpu())\n",
        "            var_agns.append(agn_out['log_var'].exp().cpu())\n",
        "\n",
        "            # Compute embedding similarity\n",
        "            sim = F.cosine_similarity(clf_out['embed'], agn_out['embed'])\n",
        "            similarities.append(sim.cpu())\n",
        "\n",
        "    # Concatenate all results\n",
        "    preds = torch.cat(preds).numpy()\n",
        "    labels = torch.cat(labels).numpy()\n",
        "    probs = torch.cat(probs).numpy()\n",
        "    var_clfs = torch.cat(var_clfs).numpy()\n",
        "    var_agns = torch.cat(var_agns).numpy()\n",
        "    similarities = torch.cat(similarities).numpy()\n",
        "\n",
        "    # Calculate metrics\n",
        "    results = {\n",
        "        # Standard classification metrics\n",
        "        'accuracy': accuracy_score(labels, preds),\n",
        "        'precision': precision_score(labels, preds, average='macro'),\n",
        "        'recall': recall_score(labels, preds, average='macro'),\n",
        "        'f1': f1_score(labels, preds, average='macro'),\n",
        "\n",
        "        # Uncertainty metrics\n",
        "        'var_clf_mean': var_clfs.mean(),\n",
        "        'var_clf_std': var_clfs.std(),\n",
        "        'var_agn_mean': var_agns.mean(),\n",
        "        'var_agn_std': var_agns.std(),\n",
        "        'similarity_mean': similarities.mean(),\n",
        "        'similarity_std': similarities.std(),\n",
        "\n",
        "        # Calibration metrics\n",
        "        'ece': CalibrationError(task='multiclass', \n",
        "                                num_classes=hparams['n_classes'])(torch.tensor(probs), torch.tensor(labels)).item()\n",
        "    }\n",
        "\n",
        "    # Variance correlation analysis\n",
        "    results['var_correlation'] = np.corrcoef(var_clfs.flatten(), var_agns.flatten())[0,1]\n",
        "\n",
        "    return results, preds, similarities\n",
        "\n",
        "def compute_confidence(logits, var=None):\n",
        "    \"\"\"Enhanced confidence computation with uncertainty awareness\"\"\"\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    conf, _ = torch.max(probs, dim=1)\n",
        "\n",
        "    if var is not None:  # Adjust confidence by uncertainty\n",
        "        conf *= torch.exp(-var)  # Higher variance reduces confidence\n",
        "    return conf\n",
        "\n",
        "def compute_certainty(clf_out, agn_out, model_type, num_classes):\n",
        "    \"\"\"Uncertainty quantification combining both encoders\"\"\"\n",
        "    if model_type == 'B':\n",
        "        return torch.sigmoid(clf_out[:, num_classes:]).squeeze()\n",
        "    elif model_type in ['N', 'ProCoSphere']:\n",
        "        # Combine uncertainties from both encoders\n",
        "        var_clf = clf_out['log_var'].exp()\n",
        "        var_agn = agn_out['log_var'].exp()\n",
        "        return 1 / (1 + var_clf + var_agn)  # Combined certainty\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model type: {model_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_embeddings_and_predictions(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_agn_embeds = []\n",
        "    all_clf_embeds = []\n",
        "    all_clf_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x_clf = batch['clf_view'].to(device)\n",
        "            x_agn = batch['agn_view'].to(device)\n",
        "            y = batch['target'].to(device)\n",
        "            \n",
        "            # Get model outputs\n",
        "            clf_out, agn_out = model(x_clf, x_agn)\n",
        "            \n",
        "            # Store embeddings and predictions\n",
        "            all_agn_embeds.append(agn_out['embed'].cpu().numpy())\n",
        "            all_clf_embeds.append(clf_out['embed'].cpu().numpy())\n",
        "            all_clf_preds.append(clf_out['logits'].argmax(1).cpu().numpy())\n",
        "            all_labels.append(y.cpu().numpy())\n",
        "    \n",
        "    return (\n",
        "        np.concatenate(all_agn_embeds),\n",
        "        np.concatenate(all_clf_embeds),\n",
        "        np.concatenate(all_clf_preds),\n",
        "        np.concatenate(all_labels)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAPUCxThGgsQ"
      },
      "source": [
        "### Visualization\n",
        "Note: needs collection of the loss values for the each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gznqmB6aQbEf"
      },
      "outputs": [],
      "source": [
        "# Plot image samples with top loss values\n",
        "def top_losses_vis(vis_params, images, preds, labels, losses):\n",
        "    num_imgs = vis_params['num_samples']\n",
        "    top_loss_indices = np.argsort(losses)[-num_imgs:]\n",
        "\n",
        "    plt.figure(figsize=(num_imgs*2, 2))\n",
        "    for i, idx in enumerate(top_loss_indices):\n",
        "        plt.subplot(1, num_imgs, i + 1)\n",
        "        plt.imshow(images[idx].squeeze(), cmap='gray')\n",
        "        plt.title(f'True: {labels[idx]}\\nPred: {preds[idx]}\\nLoss: {losses[idx]:.2f}')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrix\n",
        "def conf_mat(figsize,class_names=None):\n",
        "    plt.figure(figsize)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KemJG62D7EJC"
      },
      "source": [
        "# Ensembling\n",
        "This approach is expected to give a robust ensemble model that leverages the diversity introduced by different seeds, potentially improving the overall accuracy on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d0C987gzzlW"
      },
      "source": [
        "## Create Dataset and Data Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQmBPmYH21V9"
      },
      "source": [
        "Initialization of the dataset, the dataloader, and the training module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jrjw6mU-z36-"
      },
      "outputs": [],
      "source": [
        "data_module = CIFAR10DataModule(hparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBfy6DZL0tii"
      },
      "source": [
        "## Train the Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjxsAXHbrGC2"
      },
      "source": [
        "Loop over different seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4qZzt5w4smrt"
      },
      "outputs": [],
      "source": [
        "# List to store predictions from each model\n",
        "all_predictions = []\n",
        "all_confidences = []\n",
        "all_certainties = []\n",
        "all_best_model_paths = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ClearML Task: created new task id=6ecbb02fcfac4435aef25779fb7312ab\n",
            "2025-05-27 12:55:01,380 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
            "ClearML results page: https://app.clear.ml/projects/ccaa059e6de442b6abe578eab9e214c8/experiments/6ecbb02fcfac4435aef25779fb7312ab/output/log\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-27 12:55:09,590 - clearml.model - INFO - Selected model id: 8515be3004ca474eae4a86e50e244266\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded worse_label from ./data/CIFAR-10_human.pt.\n",
            "Noise rate: 0.40208\n",
            "\n",
            "Active training samples: 45000/45000\n",
            "Active validation samples: 5000/5000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: UserWarning:\n",
            "\n",
            "Checkpoint directory /project/ICML-2025/saved_models exists and is not empty.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type            | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | model   | ProCoSphereDual | 8.9 M  | train\n",
            "1 | loss_fn | ProCoSphereLoss | 2      | train\n",
            "----------------------------------------------------\n",
            "8.9 M     Trainable params\n",
            "0         Non-trainable params\n",
            "8.9 M     Total params\n",
            "35.478    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/90 [00:00<?, ?it/s]                            \n",
            "Active training samples: 45000/45000\n",
            "Active validation samples: 5000/5000\n",
            "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:21<00:00,  4.10it/s, v_num=399]"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'train_model' object has no attribute 'val_mask'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb Cell 56\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m task\u001b[39m.\u001b[39mconnect(model\u001b[39m.\u001b[39mhparams)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(max_epochs\u001b[39m=\u001b[39mhparams[\u001b[39m'\u001b[39m\u001b[39mnum_epochs\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m                   callbacks\u001b[39m=\u001b[39m[checkpoint_callback_img],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m                   accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m                   devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m#\"auto\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m                   )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m             data_module,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m             ckpt_path\u001b[39m=\u001b[39;49mhparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mresume_checkpoint\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m             )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Save path for later testing\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m best_model_path \u001b[39m=\u001b[39m checkpoint_callback_img\u001b[39m.\u001b[39mbest_model_path\n",
            "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    544\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    545\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
            "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    581\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1029\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1030\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1031\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:206\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance()\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:377\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39m# call train epoch end hooks\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m# we always call callback hooks first, but here we need to make an exception for the callbacks that\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39m# monitor a metric, otherwise they wouldn't be able to monitor a key logged in\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[39m# `LightningModule.on_train_epoch_end`\u001b[39;00m\n\u001b[1;32m    376\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, monitoring_callbacks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 377\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mon_train_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    378\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, monitoring_callbacks\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    380\u001b[0m trainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_epoch_end()\n",
            "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:159\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    158\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 159\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    161\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    162\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
            "\u001b[1;32m/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb Cell 56\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=227'>228</a>\u001b[0m         new_val_mask \u001b[39m=\u001b[39m (per_val_loss \u001b[39m<\u001b[39m val_thresh)\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=228'>229</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=229'>230</a>\u001b[0m         \u001b[39m# Handle empty validation losses case\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=230'>231</a>\u001b[0m         new_val_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_mask\u001b[39m.\u001b[39mclone()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=231'>232</a>\u001b[0m         val_thresh \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=232'>233</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcontainer_10.90.120.109/project/ICML-2025/ICML-2025/UQ_CIFAR-10N_ProCoSphere.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=233'>234</a>\u001b[0m     \u001b[39m# No validation losses collected\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'train_model' object has no attribute 'val_mask'"
          ]
        }
      ],
      "source": [
        "for seed in SEEDS:\n",
        "    # Set seed for reproducibility\n",
        "    seed_everything(seed)\n",
        "\n",
        "    # Reinitialize model\n",
        "    arch, loss_fn = get_arch_and_loss(hparams)\n",
        "    fname = f'arch_{ARCHITECTURE}_loss_{LOSS_FUN}_lambda_{LAMBDA}_seed_{seed}_noise_{NOISE_TYPE}'\n",
        "\n",
        "    checkpoint_callback_img = ModelCheckpoint(\n",
        "        monitor='val_loss',\n",
        "        dirpath=CHECKPOINT_PATH,\n",
        "        filename=fname,\n",
        "        save_top_k=1,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    task = Task.init(project_name=\"ICML-2025\",\n",
        "                     task_name=fname)\n",
        "\n",
        "    model = train_model(model=arch, loss=loss_fn)\n",
        "    task.connect(model.hparams)\n",
        "\n",
        "    trainer = Trainer(max_epochs=hparams['num_epochs'],\n",
        "                      callbacks=[checkpoint_callback_img],\n",
        "                      accelerator=\"auto\", \n",
        "                      devices=1 #\"auto\"\n",
        "                      )\n",
        "    trainer.fit(model,\n",
        "                data_module,\n",
        "                ckpt_path=hparams.get('resume_checkpoint', None)\n",
        "                )\n",
        "\n",
        "    # Save path for later testing\n",
        "    best_model_path = checkpoint_callback_img.best_model_path\n",
        "    all_best_model_paths.append(best_model_path)\n",
        "\n",
        "    if seed != SEEDS[-1]:\n",
        "        task.close()\n",
        "        del[model]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UzMxx8c6pvV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYbuTthVuCDo"
      },
      "source": [
        "## Test the models and the ensemble of the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_best_model_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_predictions = []\n",
        "all_confidences = []\n",
        "all_certainties_clf = []\n",
        "all_certainties_agn = []\n",
        "all_similarities = []\n",
        "\n",
        "for best_model_path in all_best_model_paths:\n",
        "    # Rebuild model arch and loss\n",
        "    arch, loss_fn = get_arch_and_loss(hparams)\n",
        "\n",
        "    # Load best checkpoint\n",
        "    best_model = robust_load_pl_model(best_model_path,\n",
        "                                      train_model,\n",
        "                                      model=arch,\n",
        "                                      loss=loss_fn,\n",
        "                                      device=device,\n",
        "                                      )\n",
        "    best_model = best_model.to(device)\n",
        "\n",
        "    # Enable MC Dropout if needed\n",
        "    mc_samples = hparams[\"dropout\"]['mc_samples']\n",
        "    if mc_samples > 0:\n",
        "        best_model.model.train()  # Enable dropout\n",
        "    else:\n",
        "        best_model.model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    confidences = []\n",
        "    similarities = []\n",
        "    certainties_clf = []\n",
        "    certainties_agn = []\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_module.test_dataloader():\n",
        "            # Get both views from batch dictionary\n",
        "            x_clf = batch['clf_view'].to(device)\n",
        "            x_agn = batch['agn_view'].to(device)\n",
        "            y = batch['target'].to(device)\n",
        "\n",
        "            if mc_samples > 0:\n",
        "                logits_samples_clf = []\n",
        "                embed_samples_clf = []\n",
        "                embed_samples_agn = []\n",
        "                var_samples_clf = []\n",
        "                var_samples_agn = []\n",
        "                \n",
        "                for _ in range(mc_samples):\n",
        "                    # Forward pass through both encoders\n",
        "                    clf_out, agn_out = best_model(x_clf, x_agn)\n",
        "                    logits_samples_clf.append(clf_out['logits'])\n",
        "                    embed_samples_clf.append(clf_out['embed'])\n",
        "                    embed_samples_agn.append(agn_out['embed']) \n",
        "                    var_samples_clf.append(clf_out['log_var'].exp())  \n",
        "                    var_samples_agn.append(agn_out['log_var'].exp())\n",
        "\n",
        "                # Average predictions\n",
        "                logits = torch.stack(logits_samples_clf).mean(0)\n",
        "                probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "                embed_clf = torch.stack(embed_samples_clf).mean(0)\n",
        "                embed_agn = torch.stack(embed_samples_agn).mean(0)\n",
        "                var_clf = torch.stack(var_samples_clf).mean(0)\n",
        "                var_agn = torch.stack(var_samples_agn).mean(0)\n",
        "                \n",
        "            else:\n",
        "                # Single forward pass\n",
        "                clf_out, agn_out = best_model(x_clf, x_agn)\n",
        "                logits = clf_out['logits']\n",
        "                probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "                embed_clf = clf_out['embed']\n",
        "                embed_agn = agn_out['embed']\n",
        "                var_clf = clf_out['log_var'].exp()\n",
        "                var_agn = agn_out['log_var'].exp()\n",
        "\n",
        "            # Collect predictions and uncertainties\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            confidence = probs.max(1)[0]\n",
        "            similarity = F.cosine_similarity(embed_clf, embed_agn)\n",
        "\n",
        "            predictions.append(preds.cpu().numpy())\n",
        "            confidences.append(confidence.cpu().numpy())\n",
        "            similarities.append(similarity.cpu().numpy())\n",
        "\n",
        "            # Calculate variance-based certainty if using N/B loss\n",
        "            if hparams['criterion'] in ['ProCoSphere']:\n",
        "                certainties_clf.append(1.0 / (1.0 + var_clf.cpu().numpy().squeeze(-1)))\n",
        "                certainties_agn.append(1.0 / (1.0 + var_agn.cpu().numpy().squeeze(-1)))\n",
        "\n",
        "    # Post-process results\n",
        "    predictions = np.concatenate(predictions)\n",
        "    confidences = np.concatenate(confidences)\n",
        "    similarities= np.concatenate(similarities)\n",
        "    all_predictions.append(predictions)\n",
        "    all_confidences.append(confidences)\n",
        "    all_similarities.append(similarities)\n",
        "\n",
        "\n",
        "    if hparams['criterion'] in ['ProCoSphere']:\n",
        "        certainties_clf = np.concatenate(certainties_clf)\n",
        "        certainties_agn = np.concatenate(certainties_agn)\n",
        "        all_certainties_clf.append(certainties_clf)\n",
        "        all_certainties_agn.append(certainties_agn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def knn_majority_vote(agn_embeddings, clf_preds, k=5):\n",
        "    nn = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(agn_embeddings)\n",
        "    distances, indices = nn.kneighbors(agn_embeddings)\n",
        "    \n",
        "    # Exclude self-neighbor\n",
        "    indices = indices[:, 1:]\n",
        "    \n",
        "    majority_preds = []\n",
        "    for i in range(len(indices)):\n",
        "        neighbor_preds = clf_preds[indices[i]]\n",
        "        counts = np.bincount(neighbor_preds)\n",
        "        majority = np.argmax(counts)\n",
        "        majority_preds.append(majority)\n",
        "        \n",
        "    return np.array(majority_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rejection_curve(preds, confidences, labels, method):\n",
        "    \"\"\"Plots accuracy vs. uncertainty threshold (rejection curve) for model predictions.\n",
        "    \n",
        "    Args:\n",
        "        preds: Array of model predictions (class indices)\n",
        "        confidences: Array of confidence scores (0-1)\n",
        "        labels: Ground truth labels\n",
        "        method: Name of the method (for plot legend)\n",
        "    \n",
        "    Returns:\n",
        "        Matplotlib figure showing:\n",
        "        - Accuracy at different confidence thresholds\n",
        "        - Percentage of retained samples at each threshold\n",
        "    \"\"\"\n",
        "    thresholds = np.linspace(0, 1, 20)\n",
        "    accuracies = []\n",
        "    keep_ratios = []\n",
        "    \n",
        "    # Calculate accuracy and sample retention at each threshold\n",
        "    for thresh in thresholds:\n",
        "        mask = confidences >= thresh\n",
        "        if mask.sum() > 0:  # Avoid division by zero\n",
        "            acc = accuracy_score(labels[mask], preds[mask])\n",
        "        else:\n",
        "            acc = 0\n",
        "        accuracies.append(acc)\n",
        "        keep_ratios.append(mask.mean())\n",
        "    \n",
        "    # Plot configuration\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(thresholds, accuracies, label=f'Accuracy ({method})', linewidth=2)\n",
        "    plt.plot(thresholds, keep_ratios, label=f'Retention ({method})', linestyle='--')\n",
        "    \n",
        "    plt.xlabel('Confidence Threshold', fontsize=12)\n",
        "    plt.ylabel('Metric Value', fontsize=12)\n",
        "    plt.title(f'Rejection Curve: {method}', fontsize=14, pad=20)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    return plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to numpy arrays with proper shapes\n",
        "all_predictions = np.array(all_predictions)        # (num_models, num_samples)\n",
        "all_confidences = np.array(all_confidences)        # (num_models, num_samples)\n",
        "all_similarities = np.array(all_similarities)      # (num_models, num_samples)\n",
        "all_certainties_clf = np.array(all_certainties_clf) # (num_models, num_samples)\n",
        "all_certainties_agn = np.array(all_certainties_agn) # (num_models, num_samples)\n",
        "\n",
        "# Get true labels\n",
        "test_labels = np.array(data_module.cifar10_test.targets)\n",
        "\n",
        "# Initialize metrics storage\n",
        "metrics = {\n",
        "    'majority': {'preds': None, 'confidence': None},\n",
        "    'confidence': {'preds': None, 'confidence': None},\n",
        "    'similarity': {'preds': None, 'confidence': None},\n",
        "    'certainty_clf': {'preds': None, 'confidence': None},\n",
        "    'certainty_agn': {'preds': None, 'confidence': None}\n",
        "}\n",
        "\n",
        "# 1. Majority Voting\n",
        "metrics['majority']['preds'] = mode(all_predictions, axis=0)[0].squeeze()\n",
        "metrics['majority']['confidence'] = all_confidences.max(axis=0)\n",
        "\n",
        "# 2. Highest Confidence Selection\n",
        "model_indices = np.argmax(all_confidences, axis=0)\n",
        "metrics['confidence']['preds'] = all_predictions[model_indices, np.arange(all_predictions.shape[1])]\n",
        "metrics['confidence']['confidence'] = all_confidences.max(axis=0)\n",
        "\n",
        "# 3. Highest Similarity Selection\n",
        "model_indices = np.argmax(all_similarities, axis=0)\n",
        "metrics['similarity']['preds'] = all_predictions[model_indices, np.arange(all_predictions.shape[1])]\n",
        "metrics['similarity']['confidence'] = all_similarities.max(axis=0)\n",
        "\n",
        "# 4. Highest Classifier Certainty\n",
        "model_indices = np.argmax(all_certainties_clf, axis=0)\n",
        "metrics['certainty_clf']['preds'] = all_predictions[model_indices, np.arange(all_predictions.shape[1])]\n",
        "metrics['certainty_clf']['confidence'] = all_certainties_clf.max(axis=0)\n",
        "\n",
        "# 5. Highest Agnostic Certainty\n",
        "model_indices = np.argmax(all_certainties_agn, axis=0)\n",
        "metrics['certainty_agn']['preds'] = all_predictions[model_indices, np.arange(all_predictions.shape[1])]\n",
        "metrics['certainty_agn']['confidence'] = all_certainties_agn.max(axis=0)\n",
        "\n",
        "# Calculate metrics\n",
        "results = {}\n",
        "ece = CalibrationError(task='multiclass', num_classes=len(CLASS_NAMES))\n",
        "\n",
        "for method in metrics:\n",
        "    preds = metrics[method]['preds']\n",
        "    confs = metrics[method]['confidence']\n",
        "    \n",
        "    # Convert to tensors for ECE calculation\n",
        "    probs = np.zeros((len(preds), len(CLASS_NAMES)))\n",
        "    probs[np.arange(len(preds)), preds] = confs\n",
        "    \n",
        "    results[method] = {\n",
        "        'accuracy': accuracy_score(test_labels, preds),\n",
        "        'ece': ece(torch.tensor(probs), torch.tensor(test_labels)).item()\n",
        "    }\n",
        "    # plot_rejection_curve(preds, confs, test_labels, method)\n",
        "\n",
        "# Print results\n",
        "print(\"Ensemble Evaluation Results:\")\n",
        "for method, vals in results.items():\n",
        "    print(f\"\\n{method.replace('_', ' ').title()}:\")\n",
        "    print(f\"Accuracy: {vals['accuracy']:.4f}\")\n",
        "    print(f\"ECE: {vals['ece']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add to results analysis\n",
        "for method in ['certainty_clf', 'certainty_agn']:\n",
        "    corr = np.corrcoef(\n",
        "        metrics[method]['confidence'],\n",
        "        (metrics[method]['preds'] == test_labels).astype(float)\n",
        "    )[0,1]\n",
        "    results[method]['accuracy_certainty_corr'] = corr\n",
        "\n",
        "print('Uncertainty correlation = ', corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate ensemble diversity using pairwise disagreement\n",
        "disagreement = np.zeros((len(test_labels),))\n",
        "for i in range(len(test_labels)):\n",
        "    unique, counts = np.unique(all_predictions[:, i], return_counts=True)\n",
        "    disagreement[i] = 1 - counts.max()/len(all_best_model_paths)\n",
        "    \n",
        "print(f\"Average Disagreement: {disagreement.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After loading models in test loop\n",
        "agn_embeds, clf_embeds, clf_preds, labels = compute_embeddings_and_predictions(best_model, data_module.test_dataloader(), device)\n",
        "\n",
        "# Calculate original classifier accuracy\n",
        "original_acc = accuracy_score(labels, clf_preds)\n",
        "\n",
        "# Calculate k-NN majority vote accuracy\n",
        "knn_preds = knn_majority_vote(agn_embeds, clf_preds, k=100)\n",
        "knn_acc = accuracy_score(labels, knn_preds)\n",
        "\n",
        "print(f\"Original Classifier Accuracy: {original_acc:.4f}\")\n",
        "print(f\"k-NN Majority Vote Accuracy: {knn_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate k-NN majority vote accuracy\n",
        "knn_preds = knn_majority_vote(clf_embeds, clf_preds, k=100)\n",
        "knn_acc = accuracy_score(labels, knn_preds)\n",
        "\n",
        "print(f\"Original Classifier Accuracy: {original_acc:.4f}\")\n",
        "print(f\"k-NN Majority Vote Accuracy: {knn_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIqc-cspappI"
      },
      "outputs": [],
      "source": [
        "task.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualization with t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_tsne(embeddings, labels, title, class_names=CLASS_NAMES):\n",
        "    plt.figure(figsize=(15,10))\n",
        "    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
        "    embeddings_2d = tsne.fit_transform(embeddings)\n",
        "    \n",
        "    # Create color map\n",
        "    cmap = plt.get_cmap('tab10', len(class_names))\n",
        "    \n",
        "    # Plot with true labels (despite noise)\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        idx = labels == i\n",
        "        plt.scatter(embeddings_2d[idx, 0], embeddings_2d[idx, 1], \n",
        "                    c=[cmap(i)], label=class_name, alpha=0.6)\n",
        "    \n",
        "    plt.title(f't-SNE: {title}')\n",
        "    plt.xlabel('TSNE-1')\n",
        "    plt.ylabel('TSNE-2')\n",
        "    plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n",
        "def get_embeddings(model, dataloader, device):\n",
        "    clf_embeds = []\n",
        "    agn_embeds = []\n",
        "    true_labels = []\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x_clf = batch['clf_view'].to(device)\n",
        "            x_agn = batch['agn_view'].to(device)\n",
        "            y = batch['target'].to(device)\n",
        "            \n",
        "            # Get both embeddings\n",
        "            clf_out, agn_out = model(x_clf, x_agn)\n",
        "            \n",
        "            clf_embeds.append(clf_out['embed'].cpu().numpy())\n",
        "            agn_embeds.append(agn_out['embed'].cpu().numpy())\n",
        "            true_labels.append(y.cpu().numpy())\n",
        "    \n",
        "    return (np.concatenate(clf_embeds),\n",
        "            np.concatenate(agn_embeds),\n",
        "            np.concatenate(true_labels))\n",
        "\n",
        "# Get embeddings for test set\n",
        "clf_emb, agn_emb, labels = get_embeddings(best_model, data_module.test_dataloader(), device)\n",
        "\n",
        "# Random subset for visualization (faster)\n",
        "subset = np.random.choice(len(labels), 1000, replace=False)\n",
        "clf_sub = clf_emb[subset]\n",
        "agn_sub = agn_emb[subset]\n",
        "lab_sub = labels[subset]\n",
        "\n",
        "# Visualize both embedding spaces\n",
        "plot_tsne(clf_sub, lab_sub, \"Classifier Encoder Embeddings\")\n",
        "plot_tsne(agn_sub, lab_sub, \"Label-Agnostic Encoder Embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add to imports\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# After test loop, before plotting rejection curves\n",
        "def plot_tsne(features, labels, title, class_names=CLASS_NAMES):\n",
        "    \"\"\"Plot t-SNE visualization with class coloring\"\"\"\n",
        "    tsne = TSNE(n_components=2, random_state=SEED)\n",
        "    features_2d = tsne.fit_transform(features)\n",
        "    \n",
        "    plt.figure(figsize=(10,8))\n",
        "    scatter = plt.scatter(features_2d[:,0], features_2d[:,1], \n",
        "                         c=labels, cmap='tab10', alpha=0.6)\n",
        "    plt.title(title)\n",
        "    plt.legend(handles=scatter.legend_elements()[0],\n",
        "               labels=class_names,\n",
        "               title=\"Classes\")\n",
        "    plt.show()\n",
        "\n",
        "# Collect embeddings and logits from test set\n",
        "clf_embeds = []\n",
        "agn_embeds = [] \n",
        "logits = []\n",
        "probs = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in data_module.test_dataloader():\n",
        "        x_clf = batch['clf_view'].to(device)\n",
        "        x_agn = batch['agn_view'].to(device)\n",
        "        y = batch['target'].to(device)\n",
        "        \n",
        "        clf_out, agn_out = best_model(x_clf, x_agn)\n",
        "        \n",
        "        clf_embeds.append(clf_out['embed'].cpu())\n",
        "        agn_embeds.append(agn_out['embed'].cpu())\n",
        "        logits.append(clf_out['logits'].cpu())\n",
        "        probs.append(torch.softmax(clf_out['logits'], dim=1).cpu())\n",
        "        true_labels.append(y.cpu())\n",
        "\n",
        "# Concatenate all batches\n",
        "clf_embeds = torch.cat(clf_embeds).numpy()\n",
        "agn_embeds = torch.cat(agn_embeds).numpy() \n",
        "logits = torch.cat(logits).numpy()\n",
        "probs = torch.cat(probs).numpy()\n",
        "true_labels = torch.cat(true_labels).numpy()\n",
        "\n",
        "# Create visualization grid\n",
        "plt.figure(figsize=(20,16))\n",
        "\n",
        "# 1. Classifier Embeddings\n",
        "plt.subplot(221)\n",
        "plot_tsne(clf_embeds, true_labels, \"Classifier Embeddings Space\")\n",
        "plt.xlabel(\"t-SNE 1\")\n",
        "plt.ylabel(\"t-SNE 2\")\n",
        "\n",
        "# 2. Agnostic Embeddings  \n",
        "plt.subplot(222)\n",
        "plot_tsne(agn_embeds, true_labels, \"Agnostic Embeddings Space\")\n",
        "plt.xlabel(\"t-SNE 1\")\n",
        "plt.ylabel(\"t-SNE 2\")\n",
        "\n",
        "# 3. Logits Space\n",
        "plt.subplot(223)\n",
        "plot_tsne(logits, true_labels, \"Classifier Logits Space\")\n",
        "plt.xlabel(\"t-SNE 1\") \n",
        "plt.ylabel(\"t-SNE 2\")\n",
        "\n",
        "# 4. Probability Space\n",
        "plt.subplot(224)\n",
        "plot_tsne(probs, true_labels, \"Classifier Probability Space\")\n",
        "plt.xlabel(\"t-SNE 1\")\n",
        "plt.ylabel(\"t-SNE 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
