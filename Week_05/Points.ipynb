{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjxslNEvsrZJDfmtWX13Ry",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mechanics-Mechatronics-and-Robotics/CV-2025/blob/main/Week_06/Points.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Points lesson by Jeremy Howard and Fast AI](https://docs.fast.ai/vision.learner.html#vision_learner)"
      ],
      "metadata": {
        "id": "ZVqHS5xDT10q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xe9CQdPTfGe"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.vision.all import *"
      ],
      "metadata": {
        "id": "WpcaN7ciTw12"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.BIWI_HEAD_POSE)"
      ],
      "metadata": {
        "id": "4z_VWZ_uUPM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path.ls()"
      ],
      "metadata": {
        "id": "8PPvv2RoUt3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 24 directories numbered from 01 to 24 (they correspond to the different persons photographed) and a corresponding .obj file (we won’t need them here). We’ll take a look inside one of these directories:"
      ],
      "metadata": {
        "id": "zxV6ueZUUxjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(path/'01').ls()"
      ],
      "metadata": {
        "id": "ZkyHqYAoUyZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside the subdirectories, we have different frames, each of them come with an image (\\_rgb.jpg) and a pose file (\\_pose.txt). We can easily get all the image files recursively with get_image_files, then write a function that converts an image filename to its associated pose file."
      ],
      "metadata": {
        "id": "wESERjinU5Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_files = get_image_files(path)\n",
        "def img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\n",
        "img2pose(img_files[0])"
      ],
      "metadata": {
        "id": "f6CmQx_VU6C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a look at our first image:"
      ],
      "metadata": {
        "id": "vrUXKDwvVU8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im = PILImage.create(img_files[0])\n",
        "print(im.shape)\n",
        "im.to_thumb(160)"
      ],
      "metadata": {
        "id": "PthD_I9MVIE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Biwi dataset web site explains the format of the pose text file associated with each image, which shows the location of the center of the head. The details of this aren’t important for our purposes, so we’ll just show the function we use to extract the head center point:"
      ],
      "metadata": {
        "id": "YBZecd3rVjH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\n",
        "def get_ctr(f):\n",
        "    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n",
        "    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n",
        "    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n",
        "    return tensor([c1,c2])"
      ],
      "metadata": {
        "id": "xztNrDY_Vc3c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function returns the coordinates as a tensor of two items:"
      ],
      "metadata": {
        "id": "4xFNL4v_VqeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_ctr(img_files[0])"
      ],
      "metadata": {
        "id": "OpgAZnHwVrAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can pass this function to DataBlock as get_y, since it is responsible for labeling each item. We’ll resize the images to half their input size, just to speed up training a bit.\n",
        "\n",
        "One important point to note is that we should not just use a random splitter. The reason for this is that the same person appears in multiple images in this dataset — but we want to ensure that our model can generalise to people that it hasn’t seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function which returns true for just one person, resulting in a validation set containing just that person’s images.\n",
        "\n",
        "The only other difference to previous data block examples is that the second block is a PointBlock. This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images."
      ],
      "metadata": {
        "id": "om7vyp6yVxt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "biwi = DataBlock(\n",
        "    blocks=(ImageBlock, PointBlock),\n",
        "    get_items=get_image_files,\n",
        "    get_y=get_ctr,\n",
        "    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n",
        "    batch_tfms=[*aug_transforms(size=(240,320)),\n",
        "                Normalize.from_stats(*imagenet_stats)]\n",
        ")"
      ],
      "metadata": {
        "id": "p_YncCrbVs5T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dls = biwi.dataloaders(path)\n",
        "dls.show_batch(max_n=9, figsize=(8,6))"
      ],
      "metadata": {
        "id": "Ai5xNr9fWE5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have assembled our data, we can use the rest of the fastai API as usual. vision_learner works perfectly in this case, and the library will infer the proper loss function from the data:"
      ],
      "metadata": {
        "id": "kwHFjxIXWC8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn = vision_learner(dls, resnet18, y_range=(-1,1))"
      ],
      "metadata": {
        "id": "ehj-MN9kWCil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.lr_find()"
      ],
      "metadata": {
        "id": "ToGpGYUMWT6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fine_tune(1, 5e-3)"
      ],
      "metadata": {
        "id": "aRP2flYxWZYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.show_results()"
      ],
      "metadata": {
        "id": "jZNQtb2QWaLu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}